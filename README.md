# Word-embeddings

Word Embeddings – SkipGram & Neural Language Model
This project implements Word2Vec using the SkipGram model with negative sampling and a Neural Language Model to learn word embeddings. It also explores cosine similarity, dimensionality reduction (SVD), and vector arithmetic for word analogies.

Features
•	Word2Vec (SkipGram): Trains embeddings using negative sampling.
•	Neural Language Model: Predicts center words from context.
•	Cosine Similarity: Measures word relationships.
•	Dimensionality Reduction (SVD): Visualizes embeddings.
•	Word Analogies: Performs vector arithmetic on word embeddings.

Technologies Used
•	NumPy
•	Matplotlib
•	NLTK (for tokenization)
•	Scikit-learn (for SVD)
•	Gensim (for pre-trained GloVe embeddings)

